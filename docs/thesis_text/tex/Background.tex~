\chapter{Background}

\section{Motivation}
With an ever aging elderly population an increasing amount of elderly also want to live at home.
While birthrates are steadily declining in Norway, we face a challenge with an ever increasing elderly population. As life expectancy increases, so does the population who needs geriatric care, either at home, or in a geriatric facility. It does not help that interest in working for elderly care is also declining in popularity.

Asymmetry in gait after stroke. -> detect and report?



\input{tex/sec_elderly_care}
\input{tex/sec_mecs}


As we are ever growing older, and staying at home until a greater age, we need to help healthcare personell to provide their services to the people that need it the most. This will help them spend their limited time more efficiently, and possibly provide 

This project describes a system for obtaining vital data from a subject for the purposes of modelling the subjects state. It also implements methods for predicting future states based on predetermined risk factors, contextual input, and the observed subject's historical data.


Contextual information: The users contemporary environment represents situational hazards and could provoke, or lessen the possibility of, certain events. For example, a day with high temprature and humidity coupled with activity could provoke dehydration and cause a fall. If such conditions are correctly assessed, preventative measures could be taken, such as suggesting to lower the temprature, take refuge in the shade, or asking to provide a glass of water.


RGBD Sensor
Widley used already implemented robotic systems. Using this sensor will give us a plethora of different alternative robotic platforms to base the system on. There are also many preexisting datasets for activity recognition using RGBD sensor to train the system on. An RGBD sensor has also been used to extract vital information from the video stream.
The RGBD sensor is also especially developed for estimating human pose, which can help in recognizing human positure and as mentioned activity.
In addition the sensor is versatile, and can be used for mapping and navigation as well.

Jetson TX2
In our experiments the Jetson TX2 board was used to preform human detection in the RGBD image using the OpenPose software developed by the CMU Perceptual Computing Lab. 

Pose detection 2d pose detection was done with the help of the OpenPose pretrained network. We then extract the 3d keypoints by projecting and scaling a rigid skeleton onto the depth image. Unobserved points are estimated using previous data, or confined to obstructed space and the kinematics of the skeleton.

Tracking the tracking of each person in the scene is done using face recognition and a simple kalman filter to predict the next location of each person. The faces are pre-stored on the system, and only anonymized information is sent over the network.

The software also supports tracking using multiple RGBD cameras, and will yield a more precise result. 

RoI extraction is done simply by making a box that contains all detected keypoints. We do the same for respective face and chest RoIs.

Mood extraction using the Face RoI detected, we run a mood recognition algorithm on the subject. This is done on system to prevent personal information being sent over the network.

Vitals are extracted using spacio temproal methods described in MiT methods, and built upon by various others.

Human Activity Recognition was traind on various RGBD datasets.

Propose decision tree or unsupervised learning model to detect anomalies in the daily routines.

Bottom Up
We then want to be able to predict what the person will experience in the near future, AND be able to take preventative measurements so that doesn't happen.
We could imagine a decision tree, and then suggest the available descisions that does not lead to the event we're trying to avoid -- kind of like a chess bot that would always choose the maximum possibility route for winning. However the problem is creating the decision tree. We need data, and know at exactly which moment a decision is being made.
We could look at the life of the subject as a series of predetermined, reliably recognizable actions. Then, we think of the actions as edges, or decisions, that lead to new states. In each state, we would also have a set of parameters describing the persons vitals and environmental conditions. If all actions are possible in any state, we need to record the similarities beteween different peoples strings of actions. We want the likeleyhood of a person taking a certain action given a set of parameters, and wether or not that action led to a fall or another undesirable event or not.

For this a Hidden Marov Model could be used. The problem is obtaining the training data.
We could make the training data synthetically to train the model, and then retrain it using actual data when the system is live. However, this might be immoral, in that we would have to actually get the system to fail to learn anything. If the system never fails, we could be stuck in a local optimum where unneccessarily many restrictions are suggested to the subject.

\section{Robotic Operating System}

\section{Open Pose}

\begin{figure}
  \begin{floatrow}
    \ffigbox{
      \begin{tikzpicture}[
          every node/.style={draw,circle,minimum size=.06cm, inner sep=1.3pt}
        ]
        \tiny
        %% \draw[help lines, step=5mm, gray!20] (-4,-4) grid (3,4);
        
        \node[label={[label distance=-.2mm]200:{0}}] (nose) at (0,3.25) {};
        \node[label={[label distance=-.2mm]140:{1}}] (neck) at (0,2.54) {};
        
        \node[label={[label distance=-.1mm]90:{14}}] (reye) at (-.3,3.5) {};
        \node[label={[label distance=-.1mm]90:{15}}] (leye) at (.3,3.5) {};

        \node[label={[label distance=-.1mm]180:{16}}] (rear) at (-.5,3.35) {};
        \node[label={[label distance=-.1mm]0:{17}}] (lear) at (.5,3.35) {};

        \node[label={[label distance=-.2mm]140:{2}}] (rshoulder) at (-1.05,2.54) {};
        \node[label={[label distance=-.2mm]50:{5}}] (lshoulder) at (1.05,2.54) {};
        
        \node[label={[label distance=-.1mm]180:{3}}] (relbow) at (-1.36,1.06) {};
        \node[label={[label distance=-.1mm]0:{6}}] (lelbow) at (1.36,1.06) {};

        \node[label={[label distance=-.1mm]180:{4}}] (rwrist) at (-1.36,-.12) {};
        \node[label={[label distance=-.1mm]0:{7}}] (lwrist) at (1.36,-.12) {};

        \node[label={[label distance=-.2mm]140:{8}}] (rhip) at (-.78,.2) {};
        \node[label={[label distance=-.2mm]50:{11}}] (lhip) at (.78,.2) {};

        \node[label={[label distance=-.1mm]180:{9}}] (rknee) at (-1.02,-1.78) {};
        \node[label={[label distance=-.1mm]0:{12}}] (lknee) at (1.02,-1.78) {};

        \node[label={[label distance=-.1mm]180:{10}}] (rankle) at (-1.02,-3.78) {};
        \node[label={[label distance=-.1mm]0:{13}}] (lankle) at (1.02,-3.78) {};

        %% \draw[blue] (0,0) circle [radius=.06cm];

        \draw (nose) -- (neck);
        \draw (reye) -- (nose); \draw (leye) -- (nose);
        \draw (reye) -- (rear); \draw (leye) -- (lear);
        
        \draw (neck) -- (rshoulder); \draw (neck) -- (lshoulder);
        \draw (neck) -- (rhip); \draw (neck) -- (lhip);

        \draw (rshoulder) -- (relbow); \draw (lshoulder) -- (lelbow);
        \draw (rwrist) -- (relbow); \draw (lwrist) -- (lelbow);

        \draw (rhip) -- (rknee); \draw (lhip) -- (lknee);
        \draw (rknee) -- (rankle); \draw (lknee) -- (lankle);
      \end{tikzpicture}
    }
    {
      \label{fig:openpose_skeleton}
      \caption[Numbering for OpenPose's keypoint markers]{Numbering for OpenPose's keypoint markers.}
    }
    %% \end{figure}
    %% \begin{table}
    \capbtabbox{
      \begin{tabular}[H]{r l}
        ID & Name \\ \hline
        0  & Nose \\
        1  & Neck \\
        2  & Right Shoulder \\
        3  & Right Elbow \\
        4  & Right Wrist \\
        5  & Left Shoulder \\
        6  & Left Elbow \\
        7  & Left Wrist \\
        8  & Right Hip \\
        9  & Right Knee \\
        10 & Right Ankle \\
        11 & Left Hip \\
        12 & Left Knee \\
        13 & Left Ankle \\
        14 & Right Eye \\
        15 & Left Eye \\
        16 & Right Ear \\
        17 & Left Ear \\
        \hline
      \end{tabular}
    }{
      \label{tab:openpose_body_ids}
      \caption[IDs for OpenPose keypoint markers]{IDs for OpenPose's keypoint markers.}
    }
  \end{floatrow}
\end{figure}

\section{Depth perception}

\section{Structured Light}

\section{Filtering and frequency enhancement}

\section{Microsoft Kinect}

\section{Human Pose Estimation}


